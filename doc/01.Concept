Концепция сервисно-ориентированного программирования.

Все мы помним, что выполнение программы начинается с функции main().
Eсть куча кода в приложении, который готовит среду для выполнения main(), инициализируются глобальные переменные, 
устанавливаются параметры командной строки argv.

Программа последовательно выполнилась, вычислились какие-то данные, создался архив, 
вытянулись данные из репозитария, скомпилялся какой-то код.
Программа завершилась с каким-то кодом возврата, который можно словить в скрипте bash.
Особенность данной схемы - есть начало выполнения программы и есть конец.

Следовательно, стандартный компилятор C++ - это фреймворк для программ, которые выполняют разовую работу и завершаются по выполнении.

Рассмотрим иную задачу: у нас есть GUI. Там никакую работу выполнять не нужно. 
Нужно просто ожидать действий юзера в виде нажатий на мышку или клавиатуру и выполнять какие-то действия. 
Уже совсем иная архитектура. Нет никакого последовательного выполнения.
Чтобы работать с GUI используют различные фреймворки наподобие Qt или  WinAPI.
Их общая особенность - наличие eventloop - цикла, который обрабатывает события вроде нажатия на кнопку мышки.
GUI фреймворк предоставляет также возможность удобного рисования кнопок, сам рисует курсор мышки. 
Все, что остается программисту - указать фреймворку как нарисовать графические элементы и самому запрограммировать реакцию на действия юзера. Все.

Eще задача: нам нужно написать сервер.
Сервер также не выполняется разово, а висит в памяти и ожидает каких-то событий - если это http сервер, 
то он ожидает входящих запросов на каком-то порту. То есть вместо нажатия кнопки мышки как GUI, 
сервер ловит входящие запросы, вычисляет ответ и отдает его клиенту.
Как это реализовать? Первое, что приходит в голову - как сделано в GUI фреймворках. 
Eсть нечто, куда приходит событие о входящем запросе.
То есть это опять eventloop. Eventloop предоставляет программисту функцию WindowProc, в которой делается обработка входящих событий.
Разумеется, мы ее назовем не WindowProc, a, допустим, handleEvent.

Как мы назовем обьект, который содержит отдельный eventloop? А давайте назовем его "сервис".

Какие могут быть у сервиса общие свойства?
1. Наличие handleEvent
2. Нужно как-то отработать старт сервиса. При старте сервиса мы можем сделать какие-то начальные установки. 
    Сделаем это при помощи посылки в сервис евента startService.

Для реализации вебсервера нам нужно сделать так, чтобы в сервис приходил евент httpEvent::RequestIncoming.
А ответ мы делаем при помощи некоего класса HTTP::Response, которые использует данные из httpEvent::RequestIncoming для формирования ответа.

Итак мы можем полностью инкапсулировать в потроха своего фреймворка работу с сокетами, http протоколом и на выходе оставить прикладному программисту 
максимально упрощенный способ делать http server.

Мы получили сервисную архитектуру. В ней софт разбивается на множество сервисов, которые общаются между собой событиями.

Eсть ли в сервисной архитектуре функция main() ?
Eе там нет, точнее она есть где-то в потрохах фреймворка. 
Что нам нужно, для запуска нашего сервера? Нам нужно загрузить наш сервис в память и дать ему старт. 
На старте он посылает евент сервису HTTP - doListen(sockaddr)
Сам сервис HTTP создает сокет и посылает его в сервис Socket, чтобы тот его мультиплексировал, читал и писал данные.

Как проще всего стартовать наш сервис?
Указать его в конфиге фреймворка.
Делаем поле:
Start=DemoWeb
где DemoWeb имя нашего сервиса, под которым мы его зарегистрировали во фреймворке.


Свойства сервисов

1. Изоляция данных. Сервисы выполняются в виде длл, не имеют между собой никаких общих данных. Влиять на данные друг друга невозможно. 
2. Один поток eventloop. Внутри сервиса не нужно связываться с мутексами и атомиками - это гарантия от ошибок, которые трудно найти. 
    Отсутствие мутексов - это и высокое быстродействие.
    Не нужна высокая квалификация программирования, с программированием может справиться любой инженер, 
    разработавший девайс и решивший написать под него сервис.
3. Сервисы взаимодействуют исключительно при помощи событий. Это простая структура на C++, написанная определенным образом. 
    Чтобы написать новый евент, нужно скопипастить какой-то другой евент и поменять в нем члены класса на свои.


Модель сервиса
Судя по всему сервисы на базе фреймворка удовлетворяют требованию 7 левела SaaS.
Отдельные сервисы типа RPC, которые используются как сессионный транспорт - это 5 левел.


Удобство для деплоя.

Обычно делается так - делается билд вообще всех сервисов в копируется в docker image.
Имеем image для всех узлов.
Какая именно конфигурация узла запустится - это зависит от конфигурации конгига, который задается в env. Запускается нужный сервис, он
"расталкивает" другие сервисы, которым прилетают евенты и которые подгружаются on-demand.

В теории можно сделать репозитарий сервисов. Пример, нам прилетел евент к сервису, которого нет на диске. Мы лезем в репозитарий, 
выкачиваем сервис, подгружаем его, отправлаем ему евент.
Так можно сделать версионность и апдейты на лету. Мы делаем у такого же сервиса новой версии новый ID и он будет запущен как новый сервис. Без перезагрузки процесса.


Монолит
Можно слинковать все сервисы в монолит с ручной регистрацией всех и сделать статик билд, как в mtjs.
В этом случае его можно запускать на легком image типа alpine.
MTJS занимает 8 мб в бинарнике, но там уже есть линк mysql, кучи других либ. 

Eсли мы используем so, то понятно, что придется ставить все нужные либы.

Сравнение кейса мегатрона с кейсом редиса.

Мегатрон легко позволяет сделать рпц сервис в режиме сервера -  request/response.
Допустим, нам нужно сделать хранение данных key->value.
Redis - это универсальная штука, для которой нам нужно адаптировать формат хранения. Поиск в редисе - однопоточный.
Была задача по выдаче японских свечек.
Реализация на редисе - закачка в него OCHLV свечек на минутные интервалы на каждый токен. Всего 300 млн записей.
Для выдачи нужны расчеты по парама. Из редиса вытягиваются минутные свечки, Затем вычисляется соотношение.
Причем соотношение будет не совсем точным, поскольку минутные свечки - это не raw data.
Скорость - порядка 20 тыс свечек в сек.
Предварительный расчет свечек для всех токенов - 2 недели 
Потребление памяти - порядка 70 гб.

Мегатрон:
В памяти сервиса лежат raw-data.
Они апдейтятся изредка (1 раз в 30 сек).
Пара рассчитывается на лету. Точно.
Расчет идет многопоточно.
Потребление памяти - 6гб.
Итог: 2 млн свечек в сек. 240кб в сек.

Разница - 100 раз.

Сервер: 50-60 ЦПУ.

Скорость мегатрона сейчас на уровне топовоых фреймворков го (фастхттп) и раст. Он не медленнее нгинкса.
То есть он на максимуме теоретического предела производительности.
Думается, что скорость мультиплексирования сокетов на редисе будет не выше мегатрона.
При прочих равных - наличие возможности адаптивного программирования схемы хранения данных однозначно дает преимущества.
Отметим, что срок выполнения задачи по японским свечкам на мегатроне составила от недели до двух работы.


К какому уровню SaaS можно это отнести.
Мегатрон - это фреймворк. Просто набор компонентов для реализации сервиса уровня приложения.
В мегатроне есть сервисы, например, RPC реализует 5 уровень.
Конечный сервис типа свечек - это уровень приложения - 7 уровень.


Замена nginx.

Nginx - это мощная штука, которая позволяет создавать гибкую систему при помощи конфига. Конфиг нгингса в реальных условиях может
достигать мегабайтов кода.
В нем есть скрипты типа lua и прочее.
Однако если рассмотреть js в качестве замены конфига, то его использование может оказаться предпочтительнее за счет гибкости, более 
легкого программирования модулей.
Модель примерно такая же как у нгинкса - на js реализуем бизнеслогику, а сложные вычисления уводим под капот js.


Модель жизненного цикла.

1. MVP делаем на js с использованием нейросети. Использование js позволяет быстро набросать функционал, 
причем он будет работать без ошибок, не тратим силы на отладку.
2. При создании MVP приходится зачастую много менять бизнеслогику, пока не достигнем нужного результата, а его предугадать на старте
сложно.
3. После MVP имеем достаточно быстрый вариант, но с деградацией из-за js. Поэтому устоявшиеся компоненты затем переводим на C/C++ под капот js.
4. В результате имеем максимально быстрый вариант.

Что такое затраты на хостинг?

У ОЗОН датацентр 64 Мвт. Это примерно 200 тыс компов. Каждый комп стоит порядка 500 долл в мес.
Получаем, что затраты на хостинг у ОЗОН - 100млн долл в мес.
Затраты на хостинг - львиная доля затрат маркетплейса.

Ускорение софта на 10% дает экономию 10 млн долл в мес.
Поэтому очень важно иметь в голове схему ускорения после того, как сервис запущен, поскольку это деньги.

Как сейчас все делается. На этапе MVP используется node.js.
Затем все переделывается на go. По сути js используется для моделирования, а прод на go.
В этом случае есть этап перевода работающего сервиса на новую платформу. Это затратное мероприятие.
В нашей ситуации - мы переделываем только куски js кода после выкладывания в прод. Тестировать куски проще, чем все проект.

JS дает деградацию по скорости 31%. Топ на тесте hello world на js дает 550к рпс. Чистый мегатрон 800к.
Уже хорошо для старта. Однако там кода js мало, сложные вычисления можно моделировать на js, потом нужно уводить под капот js на c/c++.

Очереди событий

Мегатрон  обладает нативной системой передачей событий по сети в виде сервиса RPC.
Формат события бинарный и дает максимальную скорость сериализации.
Тест http_rpc дает под 300к рпс.

Что он делает - принимаем хттп запрос, делаем запрос на ноду, получаем ответ и даем ответ на запрос хттп.

В режиме js такой тест дает порядка 110к рпс.
Соответственно отпадает необходимость использования сторонних MQ.

Eсть система бинарного облака, которая обеспечивает высокоскоростной броадкаст события по большому количеству нодов (сейчас переделывается под блокчейн).

Особенность RPC мегатрона

Маршрутизация события. Мы можем передавать события в режиме форвардинга от нода к ноду, например, в ходе лоад балансинг, 
путь события запоминается и конечная точка имеет сделать возможность reply, в ходе чего ответное событие попадает к первому отправителю.
Это сродни react, только на уровне цепочки нодов.

Eсть даже фича, которая делает источником события c++ класс. Можно из класса послать событие по цепочке нодов, а ответ принять 
в виртуальном методе окна.
Удобно на c++, например Qt. Для NDK нужны доп костыли на уровне java/kotlin в стиле react.

Такая маршрутизация используется в первую очередь для бинарного облака. 
Нам нужно  послать евент на рут облака через несколько этажей и получить от него ответ.












